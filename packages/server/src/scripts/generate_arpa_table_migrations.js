
require('dotenv').config();

// NOTE(mbroussard): need to run with POSTGRES_URL overridden to ARPA URL since .env will have GOST URL
const {POSTGRES_URL} = process.env;

const {promisify} = require('util');
const {exec: origExec} = require('child_process');
const exec = promisify(origExec);
const knex = require('../db/connection');
const fs = require('fs/promises');
const path = require('path');

// NOTE(mbroussard): this ordering is sensitive since some of these tables have FK constraints on the others
const TABLES = [
    // TODO(mbroussard): do we want to rename any of these ARPA tables when moving into GOST?
    "reporting_periods",
    "uploads", // has FK to reporting_periods
    "arpa_subrecipients", // has FK to uploads
    "application_settings", // has FK to reporting_periods
    "projects", // has FK to reporting_periods
    "period_summaries", // has FK to reporting_periods, projects
];

function pgDumpCommandTemplate(url, tableName) {
    return `pg_dump ${url} --schema-only --no-owner --no-privileges --table=${tableName}`;
}

function multiplyString(str, n) {
    let ret = '';
    for (let i = 0; i < n; i++) {
        ret += str;
    }
    return ret;
}

const fourSpaces = '    ';
function indent(str, level, delimiter = fourSpaces, skipFirstLine = true) {
    const indentation = multiplyString(delimiter, level);
    return str
        .split('\n')
        .map((line, i) =>
            i > 0 || !skipFirstLine ? indentation + line : line
        )
        .join('\n');
}

function migrationTemplate({pgDumpOutput, pgDumpCommand, tableName, runDate}) {
    // This line outputted by pg_dump causes problems in Knex migrations, see
    // https://stackoverflow.com/a/70963201
    const searchPathConfig = "SELECT pg_catalog.set_config('search_path', '', false);";
    pgDumpOutput = pgDumpOutput.replace(
        searchPathConfig,
        `-- Line below commented out by generate_arpa_table_migrations.js because it interferes with Knex\n-- ${searchPathConfig}`
    );

    const text = `
/* eslint-disable func-names */

// This file was generated by generate_arpa_table_migrations.js on ${runDate.toISOString()}.
// Describe any manual modifications below:
//  - (none)

exports.up = function (knex) {
    return knex.schema.raw(
        // This SQL generated with the following command:
        // ${pgDumpCommand}
        \`
            ${indent(pgDumpOutput, 3)}
        \`
    );
};

exports.down = function (knex) {
    return knex.schema.dropTable('${tableName}');
};
    `;
    return text.trim() + '\n';
}

async function main() {
    const runDate = new Date();
    for (let i = 0; i < TABLES.length; i++) {
        const tableName = TABLES[i];
        const idx = String(i + 1).padStart(2, '0');

        // First run pg_dump to get the table DDL
        const pgDumpCommand = pgDumpCommandTemplate(POSTGRES_URL, tableName);
        let {stdout: pgDumpOutput} = await exec(pgDumpCommand);
        pgDumpOutput = pgDumpOutput.trim();

        // Then create a new Knex migration.
        // We include an extra incrementing number in the filename to ensure these run in the
        // order we create them since the seconds-resolution filename timestamps will all be
        // the same.
        const migrationName = `arpa_integration_create_table_${idx}_${tableName}`;
        const migrationPath = await knex.migrate.make(migrationName);
        const migrationFilename = path.basename(migrationPath);

        // Finally, write the dumped DDL into the generated migration file
        const generatedMigration = migrationTemplate({pgDumpOutput, pgDumpCommand, tableName, runDate});
        await fs.writeFile(migrationPath, generatedMigration, {flag: 'w'});

        console.log('Created migration', migrationFilename);
    }
}

if (require.main === module) {
    main();
}
